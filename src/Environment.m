classdef Environment < Configurable & Copyable & handle
  %ENVIRONMENT An environment (target system).
  %
  %   A target system to be controlled. The interface properties of the
  %   system are defined by re-implementing the method getProps() and
  %   returning a proper props struct from it (see getProps()), and
  %   possibly re-implementing also getAvailableActions(). The system
  %   dynamics are defined by implementing the methods resetState(),
  %   advanceState(), checkEndCondition(), generateVectorialState(),
  %   generateObservation(), and generateReward().
  %
  %   Creating a new Environment object and using it consists of the
  %   following steps:
  %     1. Instantiate a new environment object using the constructor.
  %     2. Some environments require a call to construct().
  %     3. Initialize the environment by calling init().
  %     4. Use the object as described below (The step cycle).
  %
  %
  %   The step cycle
  %
  %   The step cycle is considered to start from a state and observation
  %   generated by the environment. An action is then selected by the
  %   agent, and the cycle is completed when the environment computes the
  %   immediate reward based on the previous state and action. The
  %   environment then makes a state transition and the process repeats.
  %   The end state and observation of an episode are not included in
  %   observations nor in the logs.
  %
  %   The interaction between an Environment and an Agent is initiated from
  %   Trainer and controlled by EvaluatePolicy. After construction and
  %   initialization, an episode consists of the following steps:
  %     1.  [observation, actions] = environment.newEpisode();
  %     2.  agent.newEpisode();
  %     3.  while ~isempty(observation)
  %     3.1.  action = agent.step( reward, observation, actions );
  %     3.2.  [reward, observation, actions] = environment.step( action );
  %     4.  agent.step( reward, [], [] );
  
  % TODO Clean up the temporary observation log solution.
  %
  % TODO Make props again a public property. This is possible if
  % environment classes do not internally read back anything from it. (the
  % current solution is for allowing an environment to be wrapped and the
  % externally visible props to be redefined, without affecting the props
  % that are internally visible to the wrapped environment)
  %
  % TODO Put the 'actions' field back to Environment. this is possible when
  % the Tetris featurizer class has been made a fully separate wrapper
  % class instead of being based on inheritance.
  %
  % TODO Factor out all visualization stuff.
  %
  % TODO Implement getAvailableNextObservations() for speedup. (?)
  %
  % TODO Re-think the case of using action lists and having discrete
  % actions. Currently this means that the rows are index vectors. However,
  % it might be more flexible to have it mean that the actions are
  % separated into |A| blocks (as in GraphGeneric), but that each block can
  % contain whatever the type of the observation space is. A discrete
  % action space with action lists would then mean that the action vector
  % is an "index vector" that uses the observation vector as the index, and
  % a continuous action space with action lists would mean that the action
  % vector is dense (no block structure). The former would lead to no
  % generalization between actions, while the latter would introduce
  % generalization between actions.
  
  %#ok<*MANU,*INUSD>
  
  
  
  
  % properties begin
  
  
  properties
    
    % Random number stream
    rstream;
    
    % Temporary solution. TODO clean up
    observationLog;
    observationLogLength = 0;
    
    % Proxy for relaying state data to the Logger.
    loggerProxy = struct( 'lastReturn', NaN );
    
  end
  
  properties (Access=protected)
    
    % Whether an episode is running. step() can be called if this is true.
    % type: logical
    episodeRunning = false;
    
    % Cached vectorial representation of the state.
    %   (row double array) vectorialState
    vectorialState;
    
    % Cached observation vector generated for the state.
    %   (row double array) observation
    observation;
    
  end
  
  
  
  
  % methods defining the environment begin (re-implement these)
  
  
  methods (Abstract)
    
    % Returns a struct containing types and dimensions of observations and
    % actions. The inheriting class should implement this method.
    %
    % Fields:
    %
    %   (char) observationType, actionType
    %     Types of the observation and action spaces. 'c' denotes a
    %     continuous space, 'd' denotes a discrete space. For a continuous
    %     space, the corresponding state or action vector is an
    %     n-dimensional real-valued vector. For a discrete space, the
    %     corresponding vector is a sparse index vector, i.e., it is zero
    %     except for a single element that is one.
    %
    %   (int) observationDim, actionDim
    %     Dimensions of the observation and action spaces. observationDim
    %     is the length of the observation vector returned by step(). If
    %     useActionsList == false (see below), then actionDim is the length
    %     of the action vector that the environment expects to receive
    %     back. If useActionsList == true, then actionDim is the length of
    %     the action feature vectors (rows) in the actions matrix returned
    %     by step(), i.e., the width of the actions matrix.
    %
    %   (logical) useActionsList
    %     If true, then getAvailableActions() must be re-implemented and it
    %     has to return an actions matrix. In this case, the agent chooses
    %     a row from this matrix and returns its index. Otherwise
    %     getAvailableActions() must not be re-implemented and the agent is
    %     expected to return an action vector that satisfies the action
    %     definitions in this props struct. In other words: if
    %     useActionsList is false, then actionType and actionDim define the
    %     type and length of the action vector that the agent should
    %     return, and if useActionsList is true, then actionType and
    %     actionDim define the type and length of the rows of the actions
    %     matrix and the agent should return a row index to that matrix.
    %     The default interpretation of the rows in the actions matrix is
    %     to consider them as feature representations of state-actions.
    %     Consequently, it should be possible to do action selection by
    %     looking only at the actions matrix while ignoring the observation
    %     vector.
    props = getProps( this );
    
  end
  
  methods (Abstract, Access=protected)
    
    % Reset the internal state.
    this = resetState( this )
    
    % Advance the internal state.
    this = advanceState( this, action )
    
    % Return 'true' if the episode end condition is met.
    ended = checkEndCondition( this )
    
    % Generate a row vectorial representation of the internal state.
    stateVec = generateVectorialState( this )
    
    % Generate a row observation vector from the internal state.
    observation = generateObservation( this )
    
    % Generate a scalar reward for the previous transition.
    reward = generateReward( this )
    
  end
  
  methods (Access=protected)
    
    function actions = getAvailableActions( this )
      % Returns a matrix that contains a row for each action that is
      % available in the current state. Each row should contain the feature
      % representation of the corresponding action. An empty matrix is
      % returned by default, indicating that the full action range
      % specified in the props struct (see getProps) is always available.
      % Re-implement this if the available actions depend on the state.
      
      actions = [];
    end
    
  end
  
  
  
  
  % external interface methods begin (consider extending some)
  
  
  methods
    
    function this = init( this, seed )
      % Initialize and reset the environment.
      %
      %   init( this, [seed] )
      %
      %   seed
      %     Random seed. This is passed directy to RandStream.
      %
      % Call this at least once before using the object. Call without
      % arguments for just resetting the environment.
      
      % no-op if just resetting
      if nargin == 1; return; end
      
      this.rstream = MexCompatibleRandStream('mt19937ar', 'seed', seed );
      
    end
    
    function [this, observation, actions] = newEpisode( this )
      % Start a new episode.
      %
      %   [this, observation, actions] = newEpisode( this )
      %
      % Overwrite the internal state by drawing a new state from the start
      % state distribution. This has to be called also in the beginning of
      % the first episode.
      %
      %   (double array) observation
      %     The observation vector generated by the new state.
      %
      %   (double matrix) actions
      %     An action matrix or the empty matrix. See getAvailableActions().
      
      % reset the internal state
      this = resetState( this );
      
      % generate vectorial state, observation and actions
      this.vectorialState = generateVectorialState( this );
      this.observation = generateObservation( this );
      
      % set the episode running flag
      this.episodeRunning = true;
      
      % clear loggerProxy data
      this.loggerProxy.lastReturn = 0;
      
      % return the observation and actions
      observation = this.observation;
      actions = getAvailableActions( this );
      
    end
    
    function [this, reward, observation, actions] = step( this, action )
      % Advance the state of the environment by one time step.
      %
      %   [this, reward, observation, actions] = step( this, action )
      %
      %   (row double array) action
      %     The action vector. If the 'actions' list from the previous step
      %     is not an empty matrix, then the action must be an index
      %     pointing to a row in that action matrix.
      %
      %   (double) reward
      %     The immediate reward obtained during the previous step.
      %
      %   (row double array) observation
      %     A constant length observation vector corresponding to the new
      %     state. An empty matrix is returned if the episode ended.
      %
      %   (double matrix) actions
      %     An action matrix or the empty matrix. See
      %     getAvailableActions().
      %
      % Entering a terminal state is signaled by returning an empty matrix
      % as the observation. Further calls to step() are not allowed after
      % this and will result in an error. Call newEpisode() to start a new
      % episode.
      
      % check that an episode is running
      assert( this.episodeRunning, 'No episode is running.' );
      
      % validate the proposed action
      validateAction( this, action );
      
      
      % advance the state and check for end condition
      this = advanceState( this, action );
      this.episodeRunning = ~checkEndCondition( this );
      
      % compute reward
      reward = generateReward( this );
      
      % relay to episodic logger
      this.loggerProxy.lastReturn = this.loggerProxy.lastReturn + reward;
      
      
      % compute the new vectorial state, observation and actions
      if this.episodeRunning
        this.vectorialState = generateVectorialState( this );
        this.observation = generateObservation( this );
        observation = this.observation;
        actions = getAvailableActions( this );
      else
        this.vectorialState = [];
        this.observation = [];
        observation = [];
        actions = [];
      end
      
    end
    
    
    function [this, data] = mexFork( this, useMex )
      % This must be called where a mex execution path (if one exists)
      % could be started.
      %
      %   (logical) useMex
      %     Whether to prepare for executing mex or Matlab code.
      %
      %   data
      %     Data for the mex implementation, or an empty matrix is useMex ==
      %     false
      
      if useMex
        
        % set episode running flag
        this.episodeRunning = true;
        
        % fill in data
        data.rstream = this.rstream;
        
      else
        mexFork( this.rstream ); data = [];
      end
      
    end
    
    function this = mexJoin( this, data )
      % This must be called where the mex execution path (if one exists)
      % could join back in.
      %
      %   data
      %     Data from the mex implementation. Set to [] if Matlab code was
      %     used.
      
      if ~isempty(data)
        % returning from a mex call
        
        % relay total return to logger (assume that the mex implementation
        % returns after each episode, but not earlier)
        this.loggerProxy.lastReturn = data.return;
        
        % temporary modification: read the observationlog. TODO clean up
        if this.observationLogLength + size(data.observationLog,1) > size(this.observationLog,1)
          this.observationLog = [ this.observationLog ;
            nan( 2 * (this.observationLogLength + size(data.observationLog,1)), size(data.observationLog,2) ) ];
        end
        inds = this.observationLogLength+1:this.observationLogLength+size(data.observationLog,1);
        this.observationLog(inds,:) = data.observationLog;
        this.observationLogLength = this.observationLogLength + size(data.observationLog,1);
        
      else
        mexJoin( this.rstream );
      end
    end
    
    
    function resetStats( this )
      % Reset statistics, if supported.
      
    end
    
    function stats = getStats( this )
      % Get statistics in the form of a struct, if supported. Returning a
      % struct with no fields signals that this feature is not supported.
      
      stats = struct();
    end
    
  end
  
  
  
  
  % public visualization interface methods begin
  
  
  methods
    
    % Get a struct containing statistics of the selected logged
    % trajectories.
    %function stats = getStats( this, trajectoryInds ); stats = []; end
    
    % Visualize everything available.
    %
    %   visualize( this, [trajectoryInds] )
    %
    % This can be called both in the cases of no logged and of logged
    % trajectories. Only the environment is visualized in the former case.
    % The optional 'trajectoryInds' is an index vector defining which
    % trajectories are to be visualized (default value should be something
    % reasonable). Overall statistics are still computed from all
    % trajectories.
    function visualize( this, trajectoryInds ); end
    
    % Visualize the environment itself while ignoring all logged episodes.
    % This method can be called even if no episodes have been run or
    % logged.
    function visualizeEnvironment( this ); end
    
    % Visualize statistics (eg, the state visitation distribution).
    %
    %   (struct) stats
    %     Optional. A stats struct from getStats(). If omitted, then a
    %     new one based on the logs from 'this' will be created.
    function stats = visualizeStats( this, stats ); stats = []; end
    
    % Visualize trajectories.
    %
    %   visualizeTrajectories( this, [trajectoryInds] )
    %
    %   (struct array) trajectoryInds
    %     Optional. An index vector defining which trajectories are to be
    %     visualized (default value should be something reasonable). The
    %     special case of one time instant (matrices have only one row)
    %     should be well supported to enable inspection of single states.
    function visualizeTrajectories( this, trajectoryInds ); end
    
    % Visualize a policy.
    %
    % The policy has to be stateless so that it can be queried in an
    % arbitrary (non-trajectory) order. The decisions should not depend on
    % the previous reward as rewards are not delivered here.
    %
    %   (Agent) agent
    %     The policy. Learning will be disabled automatically before
    %     sampling it.
    %   (int) nSamples
    %     Number of times the action for a single state is to be sampled.
    %     This can be 1 if the policy is deterministic.
    function visualizePolicy( this, agent, nSamples ); end
    
  end
  
  
  
  
  % private methods begin
  
  
  methods (Access=private)
    
    % Check, with an assertion, that the action is valid. Using assertion
    % instead of, for example, just clamping the action into accepted range
    % is used because clamping or similar operations can cause considerable
    % distortion to the action distribution.
    function validateAction( this, action )
      
      props = getProps( this );
      
      % if the 'actions' matrix is non-empty, then interpret action as a
      % row index into it. otherwise check the length of the action vector.
      assert( ( props.useActionsList && isscalar(action) ) || ...
              ( ~props.useActionsList && isvector(action) && ...
                length(action) == props.actionDim ));
      
    end
    
  end
  
end
