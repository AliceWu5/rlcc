classdef Agent < Configurable & Copyable & handle
  %AGENT The agent class.
  %
  %   A controller for controlling an environment. The interface properties
  %   of the environment are provided to the agent in the props argument of
  %   init(). Control decisions are done in the step() method. Learning can
  %   take place either in iterateActor() or in step().
  %
  %   By each step() call, the agent receives an observation, an action
  %   list and a reward from the environment. The action list can be either
  %   a matrix with each row corresponding to a featurization of each
  %   available action, or it can be an empty matrix. In the former case,
  %   the agent chooses a single action from the action list and sends the
  %   corresponding row index back to the environment. In the latter case,
  %   the agent generates an action according to the environment's property
  %   specifications (see Environment.getProps() for details).
  %
  %   Batch learning agents perform learning during iterateActor() calls.
  %   On-line learning agents perform learning during step() calls.
  
  % TODO Factor away the 'have to call super' anti-pattern, esp. from init().
  
  % TODO The observation vector in step() call is a row vector, although it
  % is documented as a column vector in interfaces of at least NAC and
  % LSPI.
  
  % TODO Do not return 'this'.
  
  
  %#ok<*MANU,*INUSD>
  
  
  
  
  properties
    
    % Random number stream. type: MexCompatibleRandStream
    rstream;
    
    % Whether learning is enabled. type: logical
    learning = false;
    
  end
  
  properties (Access=protected)
    
    % Properties of the environment (see Environment)
    props;
    
  end
  
  
  methods
    
    function this = init( this, seed, props )
      % Set the random seed and the properties of the observation and
      % action vectors and reset the agent.
      %
      %   init( this, [seed, props] )
      %
      % The 'props' struct can be taken directly from the Environment
      % object. Call without arguments for just resetting the agent.
      %
      % If this method is reimplemented in a derived classes, then this
      % base method must be called in the beginning of the
      % reimplementation.
      
      % no-op if just resetting
      if nargin == 1; return; end
      
      % store information
      this.rstream = MexCompatibleRandStream('mt19937ar', 'seed', seed );
      this.props = props;
      
    end
    
    function this = newEpisode( this )
      % Informs the agent about an episode reset.
      
    end
    
    function [this, action] = step( this, reward, observation, actions )
      % Decide on an action.
      %
      %   [this, action] = step( this, reward, observation, actions )
      %
      %   (see Environment.step() for argument types)
      %
      % The reward is zero for the first step of an episode. This method
      % will be called also once after the episode has ended, in which case
      % the reward is the final reward and the observation and actions are
      % empty matrices. Otherwise observation is a row double array (either
      % a real-valued or an index vector).
      % 
      % The actions argument may contain an action matrix or it might be an
      % empty matrix. By default, it is the result generated by
      % Environment.getAvailableActions(), but possible agent filters may
      % re-write it.
      %
      % Re-implement this and fill in the 'action' return value. If
      % 'actions' is a non-empty matrix, then the returned action must be a
      % row index into that matrix. Otherwise the returned action must
      % follow the specifications in the 'props' struct.
      
      % set this in the derived class
      action = [];
      
    end
    
    function this = iterateActor( this, stepsize )
      % Perform a policy improvement step. Stepsize is optional and is for
      % overriding the default step size. Batch mode agents should perform
      % policy improvement in a re-implementation of this method. On-line
      % learning agents can perform learning within the step()
      % implementation and ignore this method.
      %
      % This method is called (by Trainer) only if this.learning == true.
      
    end
    
    
    % Return the state value function as a column vector, or zeros(0,1) if
    % not available.
    function V = getV( this ); V = zeros(0,1); end
    
    % Return the action value or advantage function as a column vector, or
    % zeros(0,1) if not available.
    function Q = getQ( this ); Q = zeros(0,1); end
    
    % Return the policy parameters as a column vector, or zeros(0,1) if not
    % available.
    function theta = getTheta( this ); theta = zeros(0,1); end
    
    % Return the policy (action probabilities) as a column vector, or
    % zeros(0,1) if not available.
    function Pi = getPi( this ); Pi = zeros(0,1); end
    
    % Return the condition number of the gradient estimate, or zeros(0,1)
    % if not available.
    function cnd = getCond( this ); cnd = zeros(0,1); end
    
    
    function [this, data] = mexFork( this, useMex )
      % This must be called where a mex execution path (if one exists)
      % could be started.
      %
      %   (logical) useMex
      %     Whether to prepare for executing mex or Matlab code.
      %
      %   data
      %     Data for the mex implementation, or an empty matrix is useMex
      %     == false
    
      if useMex
        
        % perform episode initialization stuff, just as usual (assume mex
        % fork is before episode start)
        this = newEpisode( this );
        
        % fill in data
        data.rstream = this.rstream;
        
      else
        mexFork( this.rstream ); data = [];
      end
      
    end
    
    function this = mexJoin( this, data )
      % This must be called where the mex execution path (if one exists)
      % could join back in.
      %
      %   data
      %     Data from the mex implementation. Set to [] if Matlab code was
      %     used.
      
      if ~isempty(data)
        % returning from a mex call: no-op
        
      else
        mexJoin( this.rstream );
      end
      
    end
    
  end
  
  
end
